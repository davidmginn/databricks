# Specify the trigger event to start the build pipeline.
# In this case, new code merged into the release branch initiates a new build.
trigger:
- main

variables:
  buildConfiguration: 'Release'
  releaseBranchName: 'main'
  sourceAlias: '_davidmginn.databricks'

stages:
- stage: 'Build'
  displayName: 'Build job'
  jobs:
  - job: 'Build'
    displayName: 'Build job'
    # Specify the operating system for the agent that runs on the Azure virtual
    # machine for the build pipeline (known as the build agent). The virtual
    # machine image should match the one on the Azure Databricks cluster as
    # closely as possible. For example, Databricks Runtime 10.4 LTS runs
    # Ubuntu 20.04.4 LTS, which maps to the Ubuntu 20.04 virtual machine
    # image in the Azure Pipeline agent pool. See
    # https://learn.microsoft.com/azure/devops/pipelines/agents/hosted#software
    pool:
      vmImage: ubuntu-20.04

    # Install Python. The version of Python must match the version on the
    # Azure Databricks cluster. This pipeline assumes that you are using
    # Databricks Runtime 10.4 LTS on the cluster.
    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.8'
      inputs:
        versionSpec: 3.8

    # Install required Python modules and their dependencies. These
    # include pytest, which is needed to run unit tests on a cluster,
    # and setuptools, which is needed to create a Python wheel. Also
    # install the version of Databricks Connect that is compatible
    # with Databricks Runtime 10.4 LTS on the cluster.
    - script: |
        pip install pytest requests setuptools wheel
        pip install -U databricks-connect==10.4.*
      displayName: 'Load Python dependencies'

    # Use environment variables to pass Azure Databricks workspace and cluster
    # information to the Databricks Connect configuration function.
    - script: |
        echo "y
        $(DATABRICKS_ADDRESS)
        $(DATABRICKS_API_TOKEN)
        $(DATABRICKS_CLUSTER_ID)
        $(DATABRICKS_ORG_ID)
        $(DATABRICKS_PORT)" | databricks-connect configure
      displayName: 'Configure Databricks Connect'

    # Download the files from the designated branch in the Git remote repository
    # onto the build agent.
    - checkout: self
      persistCredentials: true
      clean: true

    # For library code developed outside of an Azure Databricks notebook, the
    # process is like traditional software development practices. You write a
    # unit test using a testing framework, such as the Python pytest module, and
    # you use JUnit-formatted XML files to store the test results.
    - script: |
        python -m pytest --junit-xml=$(Build.Repository.LocalPath)/logs/TEST-LOCAL.xml $(Build.Repository.LocalPath)/libraries/python/dbxdemo/test*.py || true
      displayName: 'Run Python unit tests for library code'

    # Publishes the test results to Azure DevOps. This lets you visualize
    # reports and dashboards related to the status of the build process.
    - task: PublishTestResults@2
      inputs:
        testResultsFiles: '**/TEST-*.xml'
        failTaskOnFailedTests: true
        publishRunAttachments: true

    # Package the example Python code into a Python wheel.
    - script: |
        cd $(Build.Repository.LocalPath)/libraries/python/dbxdemo
        python3 setup.py sdist bdist_wheel
        ls dist/
      displayName: 'Build Python Wheel for Libs'

    # Generate the deployment artifacts. To do this, the build agent gathers
    # all the new or updated code to be deployed to the Azure Databricks
    # environment, including the sample Python notebook, the Python wheel
    # library that was generated by the build process, related release settings
    # files, and the result summary of the tests for archiving purposes.
    # Use git diff to flag files that were added in the most recent Git merge.
    # Then add the Python wheel file that you just created along with utility
    # scripts used by the release pipeline.
    # The implementation in your pipeline will likely be different.
    # The objective here is to add all files intended for the current release.
    - script: |
        git diff --name-only --diff-filter=AMR HEAD^1 HEAD | xargs -I '{}' cp --parents -r '{}' $(Build.BinariesDirectory)
        mkdir -p $(Build.BinariesDirectory)/libraries/python/libs
        cp $(Build.Repository.LocalPath)/libraries/python/dbxdemo/dist/*.* $(Build.BinariesDirectory)/libraries/python/libs
        mkdir -p $(Build.BinariesDirectory)/cicd-scripts
        cp $(Build.Repository.LocalPath)/cicd-scripts/*.* $(Build.BinariesDirectory)/cicd-scripts
        mkdir -p $(Build.BinariesDirectory)/notebooks
        cp $(Build.Repository.LocalPath)/notebooks/*.* $(Build.BinariesDirectory)/notebooks
        mkdir -p $(Build.BinariesDirectory)/notebooks/project_a
        cp $(Build.Repository.LocalPath)/notebooks/project_a/*.* $(Build.BinariesDirectory)/notebooks/project_a
        mkdir -p $(Build.BinariesDirectory)/notebooks/project_b
        cp $(Build.Repository.LocalPath)/notebooks/project_b/*.* $(Build.BinariesDirectory)/notebooks/project_b
        mkdir -p $(Build.BinariesDirectory)/notebooks/project_c
        cp $(Build.Repository.LocalPath)/notebooks/project_c/*.* $(Build.BinariesDirectory)/notebooks/project_c
      displayName: 'Get Changes'

    # Create the deployment artifact and then publish it to the
    # artifact repository.
    - task: ArchiveFiles@2
      inputs:
        rootFolderOrFile: '$(Build.BinariesDirectory)'
        includeRootFolder: false
        archiveType: 'zip'
        archiveFile: '$(Build.ArtifactStagingDirectory)/$(buildConfiguration)/$(Build.BuildId).zip'
        replaceExistingArchive: true

    #- task: PublishBuildArtifacts@1
    #  inputs:
    #    ArtifactName: 'DatabricksBuild'
    
    - publish: '$(Build.ArtifactStagingDirectory)'
      artifact: drop

- stage: 'Dev'
  displayName: 'Deploy to the dev environment'
  dependsOn: Build
  condition:  succeeded()
  jobs:
  - deployment: Deploy
    environment: dev
    variables:
    - group: Release
    pool:
      vmImage: 'ubuntu-20.04'
    strategy:
      runOnce:
        deploy:
          steps:
          - download: current
            artifact: drop
          - task: UsePythonVersion@0
            displayName: 'Use Python 3.8'
            inputs:
              versionSpec: 3.8
          - task: ExtractFiles@1
            displayName: 'Extract build pipeline artifact'
            inputs:
              archiveFilePatterns: '$(Pipeline.Workspace)/drop/$(buildConfiguration)/*.zip'
              destinationFolder: '$(sourceAlias)/Databricks'
          - task: Bash@3
            inputs:
              targetType: 'inline'
              script: |
                pip install databricks-cli
                pip install unittest-xml-reporting
                echo $(DATABRICKS_HOST)
                echo $(DATABRICKS_TOKEN)
            displayName: 'Install Databricks CLI and unittest XML reporting'
            # Use environment variables to pass Azure Databricks workspace and cluster
            # information to the Databricks Connect configuration function.
          - script: |
              echo "$(DATABRICKS_HOST)
              $(DATABRICKS_TOKEN)" | databricks configure --token
            displayName: 'Configure Databricks'
          # - bash: |
          #     pip install databricks-cli
          #     pip install unittest-xml-reporting
          - bash: 'databricks workspace import --language=PYTHON --format=SOURCE --overwrite "$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/notebooks/dbxdemo-notebook.py" "/Shared/dbxdemo-notebook.py"'
            displayName: 'Copy notebook to workspace'
          - bash: 'databricks workspace import_dir  --overwrite "$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/notebooks/project_a" "/ProjectA"'
            displayName: 'Copy Project A to workspace'
          - bash: 'databricks workspace import_dir  --overwrite "$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/notebooks/project_b" "/ProjectB"'
            displayName: 'Copy Project B to workspace'
          - bash: 'databricks workspace import_dir  --overwrite "$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/notebooks/project_c" "/ProjectC"'
            displayName: 'Copy Project C to workspace'
          - bash: 'databricks fs cp  --overwrite "$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/libraries/python/libs/dbxdemo-0.1.0-py3-none-any.whl" "dbfs:/libraries/python/libs/dbxdemo-0.1.0-py3-none-any.whl"'
            displayName: 'Copy Python wheel to workspace'
          - task: PythonScript@0
            displayName: 'Install Python wheel on cluster'
            inputs:
              scriptPath: '$(sourceAlias)/Databricks/cicd-scripts/installWhlLibrary.py'
              arguments: '--shard=$(DATABRICKS_HOST) --token=$(DATABRICKS_TOKEN) --clusterid=$(DATABRICKS_CLUSTER_ID) --libs=$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/libraries/python/libs/ --dbfspath=/libraries/python/libs'
          - script: |
              mkdir -p $(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/logs/json
              mkdir -p $(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/logs/xml
              pip install pytest requests
            displayName: 'Create integration tests directories'
          - task: PythonScript@0
            displayName: 'Run notebook'
            inputs:
              scriptPath: '$(sourceAlias)/Databricks/cicd-scripts/executenotebook.py'
              arguments: '--shard=$(DATABRICKS_HOST) --token=$(DATABRICKS_TOKEN) --clusterid=$(DATABRICKS_CLUSTER_ID) --localpath=$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/notebooks --workspacepath=/Shared --outfilepath=$(System.ArtifactsDirectory)/$(sourceAlias)/Databricks/logs/json'
          - task: PythonScript@0
            displayName: 'Run a Python script'
            inputs:
              scriptPath: '$(sourceAlias)/Databricks/cicd-scripts/evaluatenotebookruns.py'
          - task: PublishTestResults@2
            displayName: 'Publish Test Results **/TEST-*.xml'




          
